{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESC180 Project 3\n",
    "# David Guo, Hanson Liu\n",
    "# Emerson Grabke\n",
    "# 4 December 2021\n",
    "\n",
    "import math\n",
    "\n",
    "def norm(vec):\n",
    "  '''Helper function which returns the norm of a vector stored as a dictionary, as described in the handout for Project 3.\n",
    "  '''\n",
    "  sum_of_squares = 0.0\n",
    "  for x in vec:\n",
    "      sum_of_squares += vec[x] * vec[x]\n",
    "\n",
    "  return math.sqrt(sum_of_squares)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "  ''' Helper function to calculate the cosine similarity between predefined sparse vectors stored as dictionaries; returns a scalar between 0 and 1'''\n",
    "\n",
    "  # Define dot product variable\n",
    "  dot_uv = 0\n",
    "  # Get the norm of the vectors\n",
    "  mag_u = norm(vec1)\n",
    "  mag_v = norm(vec2)\n",
    "\n",
    "  # Implement version from handout -- only multiply if keys match\n",
    "  for i in vec1:\n",
    "    for j in vec2:\n",
    "      if i == j:\n",
    "        dot_uv += vec1[i] * vec2[j]\n",
    "\n",
    "  # Compute similarity based on formula\n",
    "  cos_similarity = dot_uv/(mag_u*mag_v)\n",
    "\n",
    "  return cos_similarity\n",
    "\n",
    "def build_semantic_descriptors(sentences):\n",
    "  '''Returns a dictionary of semantic descriptors from each sentence'''\n",
    "  # Define the dictionary to return\n",
    "  sem_dict = {}\n",
    "\n",
    "  # Iterate through the sentences of the text\n",
    "  for sentence in sentences:\n",
    "    # Iterate through word of the sentence\n",
    "    for word in sentence:\n",
    "      # First check if the word has already been added to the dictionary -- we don't want duplicates\n",
    "      if word not in sem_dict:\n",
    "      # Create the ssemantic descriptor dictionary for the word\n",
    "        sem_dict[word] = {}\n",
    "\n",
    "      # Iterate through the sentence again and compare each word with the 'target' word w_i\n",
    "      for tracker in sentence:\n",
    "        # The tracker can't equal the word\n",
    "        if tracker != word:\n",
    "          # Create an entry in the dict for the tracker if none exists yet so we don't get an error we spend 3 hours and 5 coffees trying to fix fucking hell\n",
    "          if tracker not in sem_dict[word]:\n",
    "            sem_dict[word][tracker] = 0\n",
    "\n",
    "          # Increment the count of same-sentence occurence for specified word and tracker\n",
    "          sem_dict[word][tracker] += 1\n",
    "\n",
    "  return sem_dict\n",
    "\n",
    "def build_semantic_descriptors_from_files(filenames):\n",
    "  '''Processess a raw text file into a list of sentences which can be input build_semantic_descriptors, and calls the functino to create a dictionary of semantic descriptors for multiple text files.'''\n",
    "  # Create sentences list to be passed to the build_semantic_descriptors method\n",
    "  sentences = []\n",
    "  # Create a list of delimiters to more easily separate out the sentences\n",
    "  delimiters = [\".\", \"!\", \"?\"]\n",
    "  # Create a list of punctuation to remove\n",
    "  punctuations = [\",\", \"-\", \"--\", \":\", \";\", \"(\",\")\", \"'\", \"\\\"\"]\n",
    "\n",
    "  rawtext = ''\n",
    "  # Iterate through thee filenames\n",
    "  for filename in filenames:\n",
    "    # Open the file\n",
    "    f = open(filename, \"r\", encoding=\"latin1\")\n",
    "    # Some processing to remove not UTF-8 characters which would interfere with our code and joining the files into one big string\n",
    "    rawtext += \"\".join(char for char in f.read() if ord(char)<128)\n",
    "\n",
    "  # Remove non delimiting punctuation\n",
    "  for punctuation in punctuations:\n",
    "    rawtext = rawtext.replace(punctuation, \"\")\n",
    "\n",
    "  # Separate string into sentences using delimiters\n",
    "  for delimiter in delimiters:\n",
    "    rawtext = rawtext.replace(delimiter, \".\")\n",
    "\n",
    "  chunks = (rawtext.lower()).split(\".\")\n",
    "  for chunk in chunks:\n",
    "    sentences += [chunk.split()]\n",
    "\n",
    "  # Remove empty sentences since they will cause errors\n",
    "  for s in sentences:\n",
    "    if len(s) == 0:\n",
    "      sentences.remove(s)\n",
    "\n",
    "  return build_semantic_descriptors(sentences)\n",
    "\n",
    "def most_similar_word(word, choices, semantic_descriptors, similarity_fn):\n",
    "  '''Returns the most similar choice from a list of choices to the specified word using a dict of semantic descriptors, and similarity function cosine similarity'''\n",
    "\n",
    "  # List of similarities between choice i and the word\n",
    "  similarities = []\n",
    "\n",
    "  # Iterate through the choices, get the semantic similarity\n",
    "  for choice in choices:\n",
    "    # Check if both the word and the choice are in the semantic descriptors dictionary\n",
    "    if word in semantic_descriptors and choice in semantic_descriptors:\n",
    "      # Add the computed similarity score\n",
    "      similarities.append(similarity_fn(semantic_descriptors[word], semantic_descriptors[choice]))\n",
    "\n",
    "    # Add\n",
    "    else:\n",
    "      similarities.append(-1)\n",
    "\n",
    "  # Return the first instance of the max similarity, effectively np.argmax(), but since there is no argmax function for Python lists for whatever reason, we have to find a workaround\n",
    "  return choices[max(zip(similarities, range(len(similarities))))[1]]\n",
    "\n",
    "def run_similarity_test(filename, semantic_descriptors, similarity_fn):\n",
    "  '''Computes the accuracy of the get most similar word function for a synonym test described in Project 3 handout'''\n",
    "\n",
    "  # Define variables\n",
    "  computed_similarity_answers = []\n",
    "  correct_answers = []\n",
    "  num_correct = 0\n",
    "\n",
    "  # Open text file and readlines\n",
    "  with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "  # Get the word, correct word, and choices\n",
    "  for set in text:\n",
    "    set = (set.replace(\"\\n\", \"\")).split()\n",
    "\n",
    "    word = set[0]\n",
    "    correct = set[1]\n",
    "    choices = set[2:]\n",
    "\n",
    "    # Get most similar word\n",
    "    computed_similarity_answers.append(most_similar_word(word, choices, semantic_descriptors, similarity_fn))\n",
    "    # Get correct answer at location\n",
    "    correct_answers.append(correct)\n",
    "\n",
    "  # Calculate total number of correct answers\n",
    "  for i, j in zip(computed_similarity_answers, correct_answers):\n",
    "    if i == j:\n",
    "      num_correct += 1\n",
    "\n",
    "  # Calculate accuracy\n",
    "  accuracy = float(num_correct/len(text) * 100)\n",
    "\n",
    "  return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"/Users/yiyixu/Documents/First Year/Intro to Computer Programming/projects/project 3/war_and_peace.txt\",\n",
    "    \"/Users/yiyixu/Documents/First Year/Intro to Computer Programming/projects/project 3/swanns_way.txt\",\n",
    "]\n",
    "semantic_descriptors = build_semantic_descriptors_from_files(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.5 of the guesses were correct\n"
     ]
    }
   ],
   "source": [
    "res = run_similarity_test(\"/Users/yiyixu/Documents/First Year/Intro to Computer Programming/projects/project 3/test.txt\", semantic_descriptors, cosine_similarity)\n",
    "print(res, \"of the guesses were correct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
